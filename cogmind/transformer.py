from sympy import O
import torch
import torch.nn as nn
from .encoder import TransformerEncoder
from .decoder import TransformerDecoder
from .embedding import Embeddings, SharedEmbedding

class Transformer(nn.Module):
    """
    å®Œæ•´çš„Transformeræ¨¡å‹ - Encoder-Decoderæ¶æ„
    
    Args:
        src_vocab_size: æºè¯­è¨€è¯æ±‡è¡¨å¤§å°
        tgt_vocab_size: ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°
        d_model: æ¨¡å‹ç»´åº¦
        nhead: æ³¨æ„åŠ›å¤´æ•°é‡
        num_encoder_layers: ç¼–ç å™¨å±‚æ•°
        num_decoder_layers: è§£ç å™¨å±‚æ•°
        dim_feedforward: å‰é¦ˆç½‘ç»œç»´åº¦
        dropout: dropoutæ¯”ç‡
        activation: æ¿€æ´»å‡½æ•°
        share_embedding: æ˜¯å¦å…±äº«ç¼–ç å™¨è§£ç å™¨åµŒå…¥æƒé‡
    """
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation="relu", share_embedding=False):
        super().__init__()
        self.d_model = d_model
        self.src_vocab_size = src_vocab_size
        self.tgt_vocab_size = tgt_vocab_size

        # åˆ›å»ºåµŒå…¥å±‚
        self.src_embedding = Embeddings(src_vocab_size, d_model, dropout=dropout)

        if share_embedding and src_vocab_size == tgt_vocab_size:
            # å…±äº«æƒé‡ï¼ˆé€‚ç”¨äºç±»ä¼¼è¯­è¨€çš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬æ‘˜è¦ï¼‰
            self.tgt_embedding = self.src_embedding
        else:
            # ç‹¬ç«‹çš„åµŒå…¥å±‚ï¼ˆé€‚ç”¨äºæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ï¼‰
            self.tgt_embedding = Embeddings(tgt_vocab_size, d_model, dropout=dropout)

        # åˆ›å»ºç¼–ç å™¨
        from .encoder import TransformerEncoderLayer, TransformerEncoder
        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)

        # åˆ›å»ºè§£ç å™¨
        from .decoder import TransformerDecoderLayer, TransformerDecoder
        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers)

        # è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)
        
        # å¦‚æœå…±äº«åµŒå…¥ï¼Œå°†è¾“å‡ºæŠ•å½±å±‚çš„æƒé‡ä¸ç›®æ ‡åµŒå…¥å±‚ç»‘å®š
        if share_embedding and src_vocab_size == tgt_vocab_size:
            self.output_projection.weight = self.tgt_embedding.token_embedding.embedding.weight
        
        self._rest_parameters()

    def _rest_parameters(self):
        """åˆå§‹åŒ–å‚æ•°"""
        # ä½¿ç”¨Xavieråˆå§‹åŒ–çº¿æ€§å±‚
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            src: æºåºåˆ—è¯ç´¢å¼• [batch_size, src_len]
            tgt: ç›®æ ‡åºåˆ—è¯ç´¢å¼• [batch_size, tgt_len]  
            src_mask: æºåºåˆ—æ©ç 
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç ï¼ˆå› æœæ©ç ï¼‰
            memory_mask: ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æ©ç 
            
        Returns:
            è¾“å‡ºlogits [batch_size, tgt_len, tgt_vocab_size]
        """
        # 1. æºåºåˆ—åµŒå…¥å’Œç¼–ç 
        src_embedded = self.src_embedding(src) # [batch_size, src_len, d_model]
        memory = self.encoder(src_embedded, src_mask)# [batch_size, src_len, d_model]

        # 2. ç›®æ ‡åºåˆ—åµŒå…¥
        tgt_embedded = self.tgt_embedding(tgt) # [batch_size, tgt_len, d_model]

        # 3.è§£ç 
        decoder_output = self.decoder(tgt_embedded, memory, tgt_mask, memory_mask) # [batch_size, tgt_len, d_model]

        # 4.æŠ•å½±è¾“å‡º
        output = self.output_projection(decoder_output)# [batch_size, tgt_len, tgt_vocab_size]

        return output

def test_transformer():
    """æµ‹è¯•å®Œæ•´Transformeræ¨¡å‹"""
    print("=== æµ‹è¯•å®Œæ•´Transformeræ¨¡å‹ ===")
    
    # é…ç½®å‚æ•°
    src_vocab_size = 1000
    tgt_vocab_size = 1200
    d_model = 32
    nhead = 4
    num_encoder_layers = 2
    num_decoder_layers = 2
    batch_size, src_len, tgt_len = 2, 6, 4
    # åˆ›å»ºTransformeræ¨¡å‹
    transformer = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=d_model,
        nhead=nhead,
        num_encoder_layers=num_encoder_layers,
        num_decoder_layers=num_decoder_layers
    )
    
    print(f"æ¨¡å‹é…ç½®:")
    print(f"  æºè¯æ±‡è¡¨: {src_vocab_size}")
    print(f"  ç›®æ ‡è¯æ±‡è¡¨: {tgt_vocab_size}") 
    print(f"  æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"  æ³¨æ„åŠ›å¤´: {nhead}")
    print(f"  ç¼–ç å™¨å±‚æ•°: {num_encoder_layers}")
    print(f"  è§£ç å™¨å±‚æ•°: {num_decoder_layers}")
      
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    src_tokens = torch.randint(0, src_vocab_size, (batch_size, src_len))
    tgt_tokens = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))
    
    print(f"\nè¾“å…¥æ•°æ®:")
    print(f"  æºåºåˆ—å½¢çŠ¶: {src_tokens.shape}")
    print(f"  ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt_tokens.shape}")
    
    # å‰å‘ä¼ æ’­
    output = transformer(src_tokens, tgt_tokens)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shape = (batch_size, tgt_len, tgt_vocab_size)
    assert output.shape == expected_shape, f"æœŸæœ›{expected_shape}, å®é™…{output.shape}"
    
    print("âœ… å®Œæ•´Transformeræ¨¡å‹æµ‹è¯•é€šè¿‡!")
    return output

if __name__ == "__main__":
    test_transformer()
    print("\nğŸ‰ å®Œæ•´Transformeræ¶æ„å®ç°å®Œæˆï¼æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªçœŸæ­£çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼")