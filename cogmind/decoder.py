import torch
import torch.nn as nn
from .attention import MultiHeadAttention
from .feed_forward import FeedForward
from .residual import PreNormResidual

class TransformerDecoderLayer(nn.Module):
    """
    Transformerè§£ç å™¨å±‚ - ç”Ÿæˆç›®æ ‡åºåˆ—çš„æ ¸å¿ƒç»„ä»¶
    
    åŒ…å«ä¸‰ä¸ªå­å±‚ï¼š
    1. æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    2. ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–  
    3. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    
    Args:
        d_model: æ¨¡å‹ç»´åº¦
        nhead: æ³¨æ„åŠ›å¤´çš„æ•°é‡
        dim_feedforward: å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦
        dropout: dropoutæ¯”ç‡
        activation: å‰é¦ˆç½‘ç»œæ¿€æ´»å‡½æ•°
    """
    def __init__(self, d_model, nhead, dim_feedforward=None, dropout=0.1, activation="relu"):
        super().__init__()
        if dim_feedforward is None:
            dim_feedforward = d_model * 4
        self.d_model = d_model
        self.nhead = nhead

        # ç¬¬ä¸€ä¸ªå­å±‚ï¼šæ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, causal=True)
        self.residual1 = PreNormResidual(d_model, dropout)

        # ç¬¬äºŒä¸ªå­å±‚ï¼šç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout=dropout)
        self.residual2 = PreNormResidual(d_model, dropout)

        # ç¬¬ä¸‰ä¸ªå­å±‚ï¼šå‰é¦ˆç½‘ç»œ
        self.ffn = FeedForward(d_model, dim_feedforward, dropout, activation)
        self.residual3 = PreNormResidual(d_model, dropout)

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            tgt: ç›®æ ‡åºåˆ— [batch_size, tgt_len, d_model]
            memory: ç¼–ç å™¨è¾“å‡º [batch_size, src_len, d_model]
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç ï¼ˆç”¨äºæ©ç è‡ªæ³¨æ„åŠ›ï¼‰
            memory_mask: æºåºåˆ—æ©ç ï¼ˆç”¨äºç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼‰
            
        Returns:
            è§£ç åçš„åºåˆ— [batch_size, tgt_len, d_model]
        """
        # ç¬¬ä¸€ä¸ªå­å±‚ï¼šæ©ç å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        def self_attn_sublayer(x):
            output, self_attn_weights = self.self_attn(x, x, x, tgt_mask)
            return output
        
        tgt = self.residual1(tgt, self_attn_sublayer)

        # ç¬¬äºŒä¸ªå­å±‚ï¼šç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        def cross_attn_sublayer(x):
            output, cross_attn_weights = self.cross_attn(x, memory, memory, memory_mask)
            return output
        
        tgt = self.residual2(tgt, cross_attn_sublayer)

        # ç¬¬ä¸‰ä¸ªå­å±‚ï¼šå‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        tgt = self.residual3(tgt, self.ffn)
        return tgt

class TransformerDecoder(nn.Module):
    """
    Transformerè§£ç å™¨ - å †å å¤šä¸ªè§£ç å™¨å±‚
    
    Args:
        decoder_layer: è§£ç å™¨å±‚å®ä¾‹
        num_layers: è§£ç å™¨å±‚æ•°é‡
    """
    def __init__(self, decoder_layer, num_layers):
        super().__init__()
        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])
        self.num_layers = num_layers

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            tgt: ç›®æ ‡åºåˆ— [batch_size, tgt_len, d_model]
            memory: ç¼–ç å™¨è¾“å‡º [batch_size, src_len, d_model]
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç 
            memory_mask: æºåºåˆ—æ©ç 
            
        Returns:
            è§£ç åçš„åºåˆ— [batch_size, tgt_len, d_model]
        """
        output = tgt

        # é€å±‚å¤„ç†
        for layer in self.layers:
            output = layer(output, memory, tgt_mask, memory_mask)
        
        return output

def test_decoder_layer():
    """æµ‹è¯•å•ä¸ªè§£ç å™¨å±‚"""
    print("=== æµ‹è¯•Transformerè§£ç å™¨å±‚ ===")
    batch_size, src_len, tgt_len, d_model, nhead = 2, 6, 4 ,12, 3
    dim_ff = 48

    # åˆ›å»ºè§£ç å™¨å±‚
    decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    tgt = torch.randn(batch_size, tgt_len, d_model)
    memory = torch.randn(batch_size, src_len, d_model)

    print(f"ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}")
    print(f"ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {memory.shape}")

    # æµ‹è¯•æ— maskæƒ…å†µ
    output = decoder_layer(tgt, memory)
    print(f"è§£ç å™¨è¾“å‡ºå½¢çŠ¶: {output.shape}")
    # éªŒè¯è¾“å…¥è¾“å‡ºå½¢çŠ¶ç›¸åŒ
    assert output.shape == tgt.shape, f"å½¢çŠ¶ä¸åŒ¹é…: {output.shape} vs {tgt.shape}"
    
    # æµ‹è¯•å¸¦maskæƒ…å†µ
    print("\n=== æµ‹è¯•å¸¦æ©ç çš„è§£ç å™¨å±‚ ===")
    tgt_mask = torch.ones(batch_size, tgt_len, tgt_len)
    # åˆ›å»ºå› æœæ©ç ï¼šåªèƒ½çœ‹åˆ°å½“å‰å’Œä¹‹å‰çš„ä½ç½®
    for i in range(tgt_len):
        tgt_mask[:, i, i+1:] = 0
        
    memory_mask = torch.ones(batch_size, tgt_len, src_len)
    memory_mask[:, :, 4:] = 0  # å±è”½æºåºåˆ—å2ä¸ªä½ç½®
    
    output_masked = decoder_layer(tgt, memory, tgt_mask, memory_mask)
    print(f"å¸¦æ©ç è¾“å‡ºå½¢çŠ¶: {output_masked.shape}")
    
    print("âœ… è§£ç å™¨å±‚æµ‹è¯•é€šè¿‡!")
    return output, output_masked


def test_transformer_decoder():
    """æµ‹è¯•å®Œæ•´çš„Transformerè§£ç å™¨ï¼ˆå¤šå±‚å †å ï¼‰"""
    print("\n=== æµ‹è¯•å®Œæ•´Transformerè§£ç å™¨ ===")
    
    batch_size, src_len, tgt_len, d_model, nhead, num_layers = 2, 8, 5, 16, 4, 2
    
    # åˆ›å»ºè§£ç å™¨å±‚æ¨¡æ¿
    decoder_layer = TransformerDecoderLayer(
        d_model=d_model,
        nhead=nhead,
        dim_feedforward=d_model * 4
    )
    
    # åˆ›å»ºå¤šå±‚è§£ç å™¨
    decoder = TransformerDecoder(decoder_layer, num_layers)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    tgt = torch.randn(batch_size, tgt_len, d_model)
    memory = torch.randn(batch_size, src_len, d_model)
    print(f"ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}")
    print(f"ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {memory.shape}")
    print(f"è§£ç å™¨å±‚æ•°: {num_layers}")
    
    # å‰å‘ä¼ æ’­
    output = decoder(tgt, memory)
    print(f"è§£ç å™¨è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # éªŒè¯å½¢çŠ¶
    assert output.shape == tgt.shape, f"å½¢çŠ¶ä¸åŒ¹é…: {output.shape} vs {tgt.shape}"
    
    print("âœ… Transformerè§£ç å™¨æµ‹è¯•é€šè¿‡!")
    return output


if __name__ == "__main__":
    test_decoder_layer()
    test_transformer_decoder()
    print("\nğŸ‰ è§£ç å™¨å®ç°å®Œæˆï¼æˆ‘ä»¬å·²ç»æ„å»ºäº†å®Œæ•´çš„Transformerè§£ç å™¨ï¼")