import torch
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from cogmind.attention import MultiHeadAttention
from cogmind.feed_forward import FeedForward
from cogmind.residual import PreNormResidual

def test_attention_with_residual():
    """æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶ä¸æ®‹å·®è¿æ¥çš„é›†æˆ"""
    print("=== æµ‹è¯•æ³¨æ„åŠ› + æ®‹å·®è¿æ¥ ===")
    
    batch_size, seq_len, d_model, num_heads = 2, 4, 8, 2
    
    # åˆ›å»ºç»„ä»¶
    attention = MultiHeadAttention(d_model, num_heads)
    residual = PreNormResidual(d_model)
    
    # åˆ›å»ºè¾“å…¥ï¼ˆè‡ªæ³¨æ„åŠ›ï¼šQ=K=Vï¼‰
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å®šä¹‰æ³¨æ„åŠ›å­å±‚
    def attention_sublayer(x):
        output, weights = attention(x, x, x)
        return output
    
    # åº”ç”¨æ®‹å·®è¿æ¥ + æ³¨æ„åŠ›
    output = residual(x, attention_sublayer)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # éªŒè¯å½¢çŠ¶ä¸å˜
    assert output.shape == x.shape, f"å½¢çŠ¶ä¸åŒ¹é…: {output.shape} vs {x.shape}"
    
    print("âœ… æ³¨æ„åŠ› + æ®‹å·®è¿æ¥æµ‹è¯•é€šè¿‡!")
    return output

def test_feedforward_with_residual():
    """æµ‹è¯•å‰é¦ˆç½‘ç»œä¸æ®‹å·®è¿æ¥çš„é›†æˆ"""
    print("\n=== æµ‹è¯•å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ ===")
    
    batch_size, seq_len, d_model = 2, 4, 8
    d_ff = 32
    
    # åˆ›å»ºç»„ä»¶
    feed_forward = FeedForward(d_model, d_ff)
    residual = PreNormResidual(d_model)
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åº”ç”¨æ®‹å·®è¿æ¥ + å‰é¦ˆç½‘ç»œ
    output = residual(x, feed_forward)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # éªŒè¯å½¢çŠ¶ä¸å˜
    assert output.shape == x.shape, f"å½¢çŠ¶ä¸åŒ¹é…: {output.shape} vs {x.shape}"
    
    print("âœ… å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥æµ‹è¯•é€šè¿‡!")
    return output

def test_complete_flow():
    """æµ‹è¯•å®Œæ•´çš„æ•°æ®æµï¼šè¾“å…¥ â†’ æ³¨æ„åŠ›+æ®‹å·® â†’ å‰é¦ˆ+æ®‹å·®"""
    print("\n=== æµ‹è¯•å®Œæ•´æ•°æ®æµ ===")
    
    batch_size, seq_len, d_model, num_heads = 2, 5, 12, 3
    d_ff = 48
    
    # åˆ›å»ºæ‰€æœ‰ç»„ä»¶
    attention = MultiHeadAttention(d_model, num_heads)
    feed_forward = FeedForward(d_model, d_ff)
    residual1 = PreNormResidual(d_model)
    residual2 = PreNormResidual(d_model)
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"åˆå§‹è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # ç¬¬ä¸€å±‚ï¼šæ³¨æ„åŠ› + æ®‹å·®
    def attention_sublayer(x):
        output, weights = attention(x, x, x)
        return output
    
    x = residual1(x, attention_sublayer)
    print(f"æ³¨æ„åŠ›+æ®‹å·®åå½¢çŠ¶: {x.shape}")
    
    # ç¬¬äºŒå±‚ï¼šå‰é¦ˆç½‘ç»œ + æ®‹å·®
    x = residual2(x, feed_forward)
    print(f"å‰é¦ˆ+æ®‹å·®åå½¢çŠ¶: {x.shape}")
    
    # æœ€ç»ˆå½¢çŠ¶åº”ä¸åˆå§‹ç›¸åŒ
    assert x.shape == (batch_size, seq_len, d_model)
    
    print("âœ… å®Œæ•´æ•°æ®æµæµ‹è¯•é€šè¿‡!")
    return x

if __name__ == "__main__":
    test_attention_with_residual()
    test_feedforward_with_residual() 
    test_complete_flow()
    print("\nğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼ç»„ä»¶å¯ä»¥ååŒå·¥ä½œï¼")