import torch
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

def test_complete_training_pipeline():
    """æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹"""
    print("=== æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹ ===")
    
    from cogmind.transformer import Transformer
    from cogmind.loss import LabelSmoothingCrossEntropy
    from cogmind.optimizer import NoamOptimizer
    from cogmind.trainer import Trainer
    from cogmind.dataset import TranslationDataset, collate_fn
    from torch.utils.data import DataLoader
    
    # é…ç½®
    vocab_size = 100
    d_model = 32
    batch_size = 4
    num_samples = 20
    
    print("1. åˆ›å»ºæ¨¡å‹...")
    model = Transformer(
        src_vocab_size=vocab_size,
        tgt_vocab_size=vocab_size,
        d_model=d_model,
        nhead=4,
        num_encoder_layers=2,
        num_decoder_layers=2
    )
    
    print("2. åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨...")
    criterion = LabelSmoothingCrossEntropy(smoothing=0.1, ignore_index=0)
    optimizer = NoamOptimizer(
        model.parameters(),
        d_model=d_model,
        warmup_steps=10,
        factor=1.0
    )
    
    print("3. åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(model, optimizer, criterion, device='cpu')
    
    print("4. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨...")
    dataset = TranslationDataset(
        num_samples=num_samples,
        src_vocab_size=vocab_size,
        tgt_vocab_size=vocab_size
    )
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        shuffle=True
    )
    
    print("5. è¿è¡Œä¸€ä¸ªè®­ç»ƒå‘¨æœŸ...")
    train_loss = trainer.train_epoch(dataloader)
    print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
    
    print("6. è¿è¡ŒéªŒè¯...")
    val_loss = trainer.validate(dataloader)
    print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
    
    print("7. æµ‹è¯•æ£€æŸ¥ç‚¹...")
    trainer.save_checkpoint('test_pipeline_checkpoint.pth')
    
    # åˆ›å»ºæ–°çš„è®­ç»ƒå™¨å¹¶åŠ è½½æ£€æŸ¥ç‚¹
    trainer2 = Trainer(model, optimizer, criterion, device='cpu')
    trainer2.load_checkpoint('test_pipeline_checkpoint.pth')
    
    # æ¸…ç†
    if os.path.exists('test_pipeline_checkpoint.pth'):
        os.remove('test_pipeline_checkpoint.pth')
    
    print("ğŸ‰ å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•é€šè¿‡!")
    return train_loss, val_loss


if __name__ == "__main__":
    test_complete_training_pipeline()