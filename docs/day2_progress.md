## day2 层归一化实现

### 核心概念
- **特征维度归一化**：对每个样本的最后一个维度独立归一化
- **稳定训练**：解决内部协变量偏移问题
- **可学习参数**：gamma(缩放)和beta(偏移)让模型自主调整


## 1. 为什么需要层归一化？
每一层的输入分布随着训练不断变化,导致训练不稳定，学习率需要很小
**解决方案:层归一化**
稳定每一层的输入分布,并且允许使用更大的学习率,加速训练收敛
## 2. 层归一化 vs 批归一化
```
特性	        批归一化 (BatchNorm)	层归一化 (LayerNorm)
归一化维度	对batch维度归一化	        对特征维度归一化
序列数据	        效果差（序列长度变化）	效果好（独立处理每个样本）
小batch	        效果差	效果好
RNN/Transformer	不适用	完美适用
```
## 3. 层归一化的数学原理
```
对于输入 x [batch_size, seq_len, d_model]:

1. 计算均值和方差（在最后一个维度）：
   mean = mean(x, dim=-1)    # [batch_size, seq_len, 1]
   var = var(x, dim=-1)      # [batch_size, seq_len, 1]

2. 归一化：
   x_norm = (x - mean) / sqrt(var + eps)

3. 缩放和偏移：
   output = gamma * x_norm + beta
```
## 4.可学习参数的作用
```
gamma (缩放)：如果归一化损失了重要信息，gamma可以恢复尺度
beta (偏移)：如果归一化偏移了重要信息，beta可以恢复位置

初始值选择：
gamma = 1：开始时不改变尺度
beta = 0：开始时不改变位置

让模型自己学习是否需要调整
```
