# day2 
## 1.层归一化实现

### 核心概念
- **特征维度归一化**：对每个样本的最后一个维度独立归一化
- **稳定训练**：解决内部协变量偏移问题
- **可学习参数**：gamma(缩放)和beta(偏移)让模型自主调整

## 2.残差连接实现

### 核心概念
- **快捷路径**：创建从输入到输出的直接连接，解决梯度消失
- **恒等映射**：让网络更容易学习恒等函数
- **Pre-Norm vs Post-Norm**：现代模型多用Pre-Norm，训练更稳定

## 3.组装完整的Transformer编码器层，理解编码器的数据流和架构设计
```输入 → 多头自注意力 → 残差连接 & 层归一化 → 前馈网络 → 残差连接 & 层归一化 → 输出```


### 1. 为什么需要层归一化？
每一层的输入分布随着训练不断变化,导致训练不稳定，学习率需要很小
**解决方案:层归一化**
稳定每一层的输入分布,并且允许使用更大的学习率,加速训练收敛
### 2. 层归一化 vs 批归一化
```
特性	        批归一化 (BatchNorm)	层归一化 (LayerNorm)
归一化维度	对batch维度归一化	        对特征维度归一化
序列数据	        效果差（序列长度变化）	效果好（独立处理每个样本）
小batch	        效果差	效果好
RNN/Transformer	不适用	完美适用
```
### 3. 层归一化的数学原理
```
对于输入 x [batch_size, seq_len, d_model]:

1. 计算均值和方差（在最后一个维度）：
   mean = mean(x, dim=-1)    # [batch_size, seq_len, 1]
   var = var(x, dim=-1)      # [batch_size, seq_len, 1]

2. 归一化：
   x_norm = (x - mean) / sqrt(var + eps)

3. 缩放和偏移：
   output = gamma * x_norm + beta
```
### 4.可学习参数的作用
```
gamma (缩放)：如果归一化损失了重要信息，gamma可以恢复尺度
beta (偏移)：如果归一化偏移了重要信息，beta可以恢复位置

初始值选择：
gamma = 1：开始时不改变尺度
beta = 0：开始时不改变位置

让模型自己学习是否需要调整
```

### 5.为什么需要残差连接？
```
梯度消失/爆炸：反向传播时梯度越来越小/大
网络越深，训练越困难
性能饱和甚至下降
```

### 6.Pre-Norm vs Post-Norm
```
特性	        Pre-Norm	        Post-Norm
公式	        x + Sublayer(LN(x))	LN(x + Sublayer(x))
训练稳定性	更好	                稍差
收敛速度         更快	                较慢
现代模型	        常用（GPT、BERT）	        原始Transformer
```

### 7.为什么Pre-Norm更稳定？
```
LayerNorm在子层之前，稳定了子层的输入分布

梯度流动更直接，减少了梯度消失
```
