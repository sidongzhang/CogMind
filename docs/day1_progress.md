# Day 1 Progress - 注意力机制实现
## 🎯 今日成就
- ✅ 实现了缩放点积注意力机制 (ScaledDotProductAttention)
- ✅ 支持因果掩码和填充掩码
- ✅ 编写了完整的测试用例

## 📚 学习要点
1. **d_k 的作用**：查询和键向量的维度，用于缩放计算
2. **QKV 理解**：Query-Key-Value 分别代表查询、键和值
3. **掩码机制**：通过设置 -inf 来屏蔽特定位置的注意力


# 今日疑问
### 1.d_k是什么
答：d_k 是键向量和查询向量的维度。
 - Q (Query)：查询向量，表示"我在找什么"
 - K (Key)：键向量，表示"我有什么信息"（可以提供的特征）
 - V (Value)：值向量，表示"我的实际内容是什么"（具有的信息）
### （额外问题）为什么需要 d_k？（简述）
    缩放因子 1/sqrt(d_k)
    为了防止点积结果过大，导致 softmax 梯度消失。

## 2.Q,K,V的维度不太理解
 [batch_size, seq_len, d_k] 的含义：
 - batch_size: 同时处理多少个样本（如2个句子）
 - seq_len: 序列长度（如每个句子4个词）
 - d_k: 每个词的向量维度（如8维）
## 3.为啥会有两次掩码
 - 第一次掩码 - 因果掩码 (Causal Mask)：
     - 用途：确保在生成文本时，每个词只能看到它之前的词，不能"偷看"未来的词
     - 场景：主要用于解码器，保证自回归生成
 - 第二次掩码 - 填充掩码 (Padding Mask)：
     - 用途：忽略填充符号（padding tokens），不让模型关注无意义的填充位置
     - 场景：处理变长序列时使用
## 4.掩码那里具体怎么实现的，为什么这样写可以实现
掩码的核心思想：在 softmax 之前，把要屏蔽的位置设为负无穷
### eg 步骤分解：
#### 1. 创建掩码矩阵:
    要屏蔽的位置为1，其他为0
#### 2. 使用masked_fill:
    把掩码为1的位置替换为 -inf
#### 3. softmax 计算：
    e^(-inf) = 0，所以这些位置的权重为0
### eg:
    scores = [ [1.2, 0.8, 0.5],   # 原始得分
           [0.9, 1.1, 0.7],
           [0.6, 0.4, 1.0] ]

    mask = [ [0, 0, 1],           # 要屏蔽第三个位置
         [0, 0, 1], 
         [0, 0, 1] ]

#### 应用掩码后：
    scores = [ [1.2, 0.8, -inf],  # 第三个位置被屏蔽
           [0.9, 1.1, -inf],
           [0.6, 0.4, -inf] ]

#### softmax 后:
    -inf 的位置权重为0