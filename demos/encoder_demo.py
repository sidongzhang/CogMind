import torch
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from cogmind.encoder import TransformerEncoderLayer, TransformerEncoder
from cogmind.positional_encoding import PositionalEncoding

def demo_complete_encoding_process():
    """æ¼”ç¤ºå®Œæ•´çš„ç¼–ç è¿‡ç¨‹"""
    print("=== å®Œæ•´ç¼–ç è¿‡ç¨‹æ¼”ç¤º ===")
    
    # é…ç½®å‚æ•°
    batch_size = 2
    src_len = 8
    d_model = 16
    nhead = 4
    num_layers = 2
    vocab_size = 100
    
    print(f"é…ç½®:")
    print(f"  batch_size: {batch_size}")
    print(f"  åºåˆ—é•¿åº¦: {src_len}")
    print(f"  æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"  æ³¨æ„åŠ›å¤´: {nhead}")
    print(f"  ç¼–ç å™¨å±‚æ•°: {num_layers}")
    
    # 1. åˆ›å»ºè¯åµŒå…¥ï¼ˆæ¨¡æ‹Ÿï¼‰
    embedding = torch.randn(vocab_size, d_model)
    
    # 2. åˆ›å»ºè¾“å…¥åºåˆ—ï¼ˆè¯ç´¢å¼•ï¼‰
    src_tokens = torch.randint(0, vocab_size, (batch_size, src_len))
    print(f"\n1. è¾“å…¥è¯ç´¢å¼•å½¢çŠ¶: {src_tokens.shape}")
    print(f"   ç¤ºä¾‹: {src_tokens[0]}")
    
    # 3. è¯åµŒå…¥
    src_embed = embedding[src_tokens]
    print(f"2. è¯åµŒå…¥åå½¢çŠ¶: {src_embed.shape}")
    
    # 4. ä½ç½®ç¼–ç 
    pos_encoder = PositionalEncoding(d_model)
    src_encoded = pos_encoder(src_embed)
    print(f"3. ä½ç½®ç¼–ç åå½¢çŠ¶: {src_encoded.shape}")
    
    # 5. åˆ›å»ºç¼–ç å™¨
    encoder_layer = TransformerEncoderLayer(d_model, nhead)
    encoder = TransformerEncoder(encoder_layer, num_layers)
    
    # 6. ç¼–ç è¿‡ç¨‹
    memory = encoder(src_encoded)
    print(f"4. ç¼–ç åå½¢çŠ¶: {memory.shape}")
    
    print(f"\nğŸ¯ æ¼”ç¤ºå®Œæˆï¼")
    print(f"   ä»è¯ç´¢å¼• {src_tokens.shape} â†’ ç¼–ç è¡¨ç¤º {memory.shape}")
    
    return memory

def demo_attention_patterns():
    """æ¼”ç¤ºæ³¨æ„åŠ›æ¨¡å¼"""
    print("\n=== æ³¨æ„åŠ›æ¨¡å¼æ¼”ç¤º ===")
    
    # åˆ›å»ºç®€å•çš„ç¼–ç å™¨å±‚
    encoder_layer = TransformerEncoderLayer(d_model=8, nhead=2)
    
    # åˆ›å»ºæœ‰æ„ä¹‰çš„è¾“å…¥åºåˆ—
    # å‡è®¾åºåˆ—: "çŒ« å–œæ¬¢ åƒ é±¼"
    src = torch.tensor([[
        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # çŒ«
        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # å–œæ¬¢
        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # åƒ
        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # é±¼
    ]])
    
    print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: {src.shape}")
    print("è¾“å…¥åºåˆ—: 4ä¸ªä¸åŒçš„one-hotå‘é‡")
    
    # è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆéœ€è¦ä¿®æ”¹EncoderLayeræ¥è¿”å›æ³¨æ„åŠ›æƒé‡ï¼‰
    output = encoder_layer(src)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print("è¾“å‡º: æ¯ä¸ªè¯éƒ½åŒ…å«äº†æ•´ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯")
    
    return output

if __name__ == "__main__":
    demo_complete_encoding_process()
    demo_attention_patterns()